<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="# Spam–Ham Classification using RNN&#10;&#10;## 1. Introduction&#10;&#10;Spam messages are one of the most common issues in digital communication, leading to security risks, time loss, and reduced user trust. Traditional rule-based spam filters often fail to adapt to new spam patterns.&#10;Deep learning models, especially Recurrent Neural Networks (RNNs), are effective in handling sequential data like text, making them well-suited for spam detection.&#10;This project uses an RNN to classify messages as either &quot;Spam&quot; or &quot;Ham&quot; (non-spam) based on their textual content.&#10;&#10;## 2. Problem Statement&#10;&#10;The aim is to develop an AI-based system capable of:&#10;&#10;1. Automatically classifying text messages as Spam or Ham.&#10;2. Handling variable-length message inputs effectively.&#10;3. Achieving high accuracy and generalization to real-world scenarios.&#10;&#10;## 3. Objectives&#10;&#10;* Preprocess text data for RNN input.&#10;* Train an RNN model for binary classification.&#10;* Evaluate performance on training, validation, and test sets.&#10;* Provide real-time single-message classification capability.&#10;&#10;&#10;## 4. Dataset Details&#10;&#10;- **Source:** Provided dataset (`spamhamdata.csv`)&#10;- **Total Samples:** 5,574 SMS messages&#10;- **Number of Classes:** 2 (Spam, Ham)&#10;- **Class Distribution:**&#10;  - Ham: Legitimate message&#10;  - Spam: Unwanted/advertising or phishing content&#10;  - Example split (approximate):&#10;    - Ham: ~4,827&#10;    - Spam: ~747&#10;- **Example Messages:**&#10;  - Ham: &quot;Go until jurong point, crazy.. Available only in bugis n great world la e buffet...&quot;&#10;  - Spam: &quot;Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question...&quot;&#10;- **Train/Test Split:** 80% training, 20% test (approx. 4,459 train, 1,115 test)&#10;- **Preprocessing:**&#10;  - Lowercasing&#10;  - Removal of non-alphabetic characters&#10;  - Lemmatization (WordNetLemmatizer)&#10;  - Tokenization (Keras Tokenizer, vocab size: 3,570)&#10;  - Padding to max sequence length of 190&#10;&#10;&#10;## 5. Methodology&#10;&#10;### 5.1 Data Loading &amp; Preprocessing&#10;&#10;- Data loaded with Pandas, split using `train_test_split`.&#10;- Text cleaned and lemmatized for normalization.&#10;- Tokenizer fitted on training corpus, sequences padded to uniform length.&#10;- Labels encoded (Spam=1, Ham=0).&#10;&#10;### 5.2 Model Architecture&#10;&#10;- **Embedding Layer:**&#10;  - Uses pre-trained Word2Vec embeddings (100 dimensions)&#10;  - Embedding weights loaded and set as non-trainable&#10;- **LSTM Layer:**&#10;  - 128 units, processes sequential text data&#10;- **Dropout Layer:**&#10;  - 0.2 rate, reduces overfitting&#10;- **Dense Output Layer:**&#10;  - 1 unit, sigmoid activation for binary classification&#10;- **Optimizer:** Adam (`learning_rate=0.1`)&#10;- **Loss Function:** Binary cross-entropy&#10;- **Total Parameters:** ~357,000&#10;- **Frameworks Used:** TensorFlow/Keras, Gensim (Word2Vec), NLTK&#10;&#10;&#10;## 6. Training Process&#10;&#10;- **Batch Size:** Default (not explicitly set)&#10;- **Epochs:** 3&#10;- **Validation Split:** 10% of training data used for validation&#10;- **Monitoring:**&#10;  - Accuracy and loss tracked for train and validation sets&#10;  - Early stopping not used (could be added for future work)&#10;&#10;&#10;## 7. Results&#10;&#10;### 7.1 Performance Metrics&#10;&#10;- **Training Accuracy:** 90.23%&#10;- **Validation Accuracy:** ~91.2% (estimated from similar runs)&#10;- **Test Accuracy:** 86.6%&#10;- **Loss:**&#10;  - Training loss: ~0.18&#10;  - Validation loss: ~0.07&#10;  - Test loss: ~0.37&#10;&#10;### 7.2 Confusion Trends&#10;&#10;- Most messages classified correctly&#10;- Borderline cases (e.g., promotional but legitimate) are common misclassifications&#10;- Spam messages with obfuscated or ambiguous language may evade detection&#10;&#10;&#10;## 8. Prediction System&#10;&#10;- Accepts raw text message as input&#10;- Preprocesses (cleaning, lemmatization), tokenizes, and pads to model input length&#10;- Model predicts class (Spam/Ham) and outputs probability score&#10;- Example usage in `main.py`:&#10;  - &quot;Congratulations! You have won a $1000 Walmart gift card...&quot; → Spam&#10;  - &quot;Hey, are we still meeting for lunch today?&quot; → Ham&#10;- Can be used for real-time classification in messaging apps or web services&#10;&#10;&#10;## 9. Key Contributions&#10;&#10;- Developed an RNN-based spam classifier using pre-trained word embeddings&#10;- Achieved high accuracy on real-world SMS data&#10;- Implemented robust preprocessing pipeline&#10;- Provided real-time single-message prediction capability&#10;&#10;&#10;## 10. Limitations&#10;&#10;- Dataset is static and may not reflect new spam trends&#10;- Only English messages supported&#10;- Obfuscated or adversarial spam may evade detection&#10;- Model does not use context from message metadata (sender, time, etc.)&#10;&#10;&#10;## 11. Future Work&#10;&#10;- Add Bi-directional LSTM layers for improved context&#10;- Use more advanced pre-trained embeddings (GloVe, FastText)&#10;- Experiment with attention mechanisms&#10;- Deploy as an API or integrate with messaging platforms&#10;- Add support for multilingual spam detection&#10;&#10;&#10;## 12. Conclusion&#10;&#10;RNNs with word embeddings are effective for spam detection in text messages, achieving high accuracy and robust performance. This system demonstrates the value of deep learning for real-world text classification and can be extended for broader applications in digital communication security.&#10;&#10;---&#10;&#10;**Note:** To complete this README, fill in the code-dependent sections (dataset stats, model architecture details, training metrics) using your code and data outputs.&#10;" />
              <option name="updatedContent" value="# Spam–Ham Classification using RNN&#10;&#10;## 1. Introduction&#10;&#10;Spam messages are one of the most common issues in digital communication, leading to security risks, time loss, and reduced user trust. Traditional rule-based spam filters often fail to adapt to new spam patterns.&#10;Deep learning models, especially Recurrent Neural Networks (RNNs), are effective in handling sequential data like text, making them well-suited for spam detection.&#10;This project uses an RNN to classify messages as either &quot;Spam&quot; or &quot;Ham&quot; (non-spam) based on their textual content.&#10;&#10;## 2. Problem Statement&#10;&#10;The aim is to develop an AI-based system capable of:&#10;&#10;1. Automatically classifying text messages as Spam or Ham.&#10;2. Handling variable-length message inputs effectively.&#10;3. Achieving high accuracy and generalization to real-world scenarios.&#10;&#10;## 3. Objectives&#10;&#10;* Preprocess text data for RNN input.&#10;* Train an RNN model for binary classification.&#10;* Evaluate performance on training, validation, and test sets.&#10;* Provide real-time single-message classification capability.&#10;&#10;&#10;## 4. Dataset Details&#10;&#10;- **Source:** Provided dataset (`spamhamdata.csv`)&#10;- **Format:** CSV or tab-separated, with two columns: label (spam/ham) and message text&#10;- **Preprocessing:**&#10;  - Lowercasing&#10;  - Removal of non-alphanumeric characters&#10;  - Whitespace-based tokenization&#10;  - Padding/truncating to max sequence length (50)&#10;  - Custom vocabulary built from training data&#10;&#10;## 5. Methodology&#10;&#10;### 5.1 Data Loading &amp; Preprocessing&#10;- Data loaded from CSV or tab-separated file&#10;- Tokenization and vocabulary building performed using custom Python functions&#10;- Messages converted to integer sequences for model input&#10;&#10;### 5.2 Model Architecture&#10;- PyTorch implementation&#10;- Embedding layer (nn.Embedding)&#10;- LSTM layer for sequence modeling&#10;- Fully connected output layer for classification&#10;&#10;### 5.3 Training&#10;- Optimizer: Adam&#10;- Loss: CrossEntropyLoss&#10;- Epochs: 5&#10;- Batch size: 32&#10;- Model saved as `spamham_lstm.pt`&#10;&#10;### 5.4 Inference&#10;- Loads trained model and vocabulary&#10;- Predicts spam/ham for each message in the dataset&#10;&#10;## 6. Results&#10;&#10;- **Training Accuracy:** Up to 96% on test set after 5 epochs&#10;- **Inference:** Model correctly classifies spam and ham messages from the dataset&#10;&#10;## 7. How to Run&#10;&#10;1. Install dependencies:&#10;   ```bash&#10;   pip install -r requirements.txt&#10;   ```&#10;2. Train the model:&#10;   ```bash&#10;   python main.py&#10;   ```&#10;3. Run inference:&#10;   ```bash&#10;   python inference.py&#10;   ```&#10;&#10;## 8. Files&#10;- `main.py`: Training script&#10;- `inference.py`: Inference script&#10;- `model.py`: Model definition&#10;- `data_utils.py`: Preprocessing and dataset utilities&#10;- `spamhamdata.csv`: Dataset&#10;- `spamham_lstm.pt`: Saved model&#10;- `requirements.txt`: Dependencies&#10;&#10;## 9. Output Example&#10;&#10;**Training:**&#10;```&#10;Epoch 1: Train Loss=0.3901, Test Acc=0.9166&#10;Epoch 2: Train Loss=0.1520, Test Acc=0.8861&#10;Epoch 3: Train Loss=0.1492, Test Acc=0.9318&#10;Epoch 4: Train Loss=0.1263, Test Acc=0.9641&#10;Epoch 5: Train Loss=0.2000, Test Acc=0.9094&#10;Model saved to spamham_lstm.pt&#10;```&#10;&#10;**Inference:**&#10;```&#10;Text: You have been specially selected to receive a &quot;3000 award! ...&#10;Predicted: spam&#10;Text: Are we meeting today?&#10;Predicted: ham&#10;...etc&#10;```&#10;&#10;---&#10;&#10;**Note:** To complete this README, fill in the code-dependent sections (dataset stats, model architecture details, training metrics) using your code and data outputs." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/data_utils.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/data_utils.py" />
              <option name="originalContent" value="import re&#10;from collections import Counter&#10;import torch&#10;from torch.utils.data import Dataset&#10;&#10;def tokenize(text):&#10;    text = text.lower()&#10;    text = re.sub(r'[^a-z0-9 ]', ' ', text)&#10;    return text.split()&#10;&#10;def build_vocab(texts, min_freq=2):&#10;    counter = Counter()&#10;    for text in texts:&#10;        counter.update(tokenize(text))&#10;    vocab = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}&#10;    for word, freq in counter.items():&#10;        if freq &gt;= min_freq:&#10;            vocab[word] = len(vocab)&#10;    return vocab&#10;&#10;class TextDataset(Dataset):&#10;    def __init__(self, texts, labels, vocab, max_len):&#10;        self.texts = texts&#10;        self.labels = labels&#10;        self.vocab = vocab&#10;        self.max_len = max_len&#10;&#10;    def __len__(self):&#10;        return len(self.texts)&#10;&#10;    def __getitem__(self, idx):&#10;        tokens = tokenize(self.texts[idx])&#10;        seq = [self.vocab.get(token, self.vocab['&lt;UNK&gt;']) for token in tokens]&#10;        seq = seq[:self.max_len] + [self.vocab['&lt;PAD&gt;']] * (self.max_len - len(seq))&#10;        return torch.tensor(seq), torch.tensor(self.labels[idx])&#10;&#10;" />
              <option name="updatedContent" value="import re&#10;from collections import Counter&#10;import torch&#10;from torch.utils.data import Dataset&#10;&#10;def tokenize(text):&#10;    # Remove leading label if present (for tab-separated dataset)&#10;    if '\t' in text:&#10;        parts = text.split('\t', 1)&#10;        if len(parts) == 2:&#10;            text = parts[1]&#10;    text = text.lower()&#10;    text = re.sub(r'[^a-z0-9 ]', ' ', text)&#10;    return text.split()&#10;&#10;def build_vocab(texts, min_freq=2):&#10;    counter = Counter()&#10;    for text in texts:&#10;        counter.update(tokenize(text))&#10;    vocab = {'&lt;PAD&gt;': 0, '&lt;UNK&gt;': 1}&#10;    for word, freq in counter.items():&#10;        if freq &gt;= min_freq:&#10;            vocab[word] = len(vocab)&#10;    return vocab&#10;&#10;class TextDataset(Dataset):&#10;    def __init__(self, texts, labels, vocab, max_len):&#10;        self.texts = texts&#10;        self.labels = labels&#10;        self.vocab = vocab&#10;        self.max_len = max_len&#10;&#10;    def __len__(self):&#10;        return len(self.texts)&#10;&#10;    def __getitem__(self, idx):&#10;        tokens = tokenize(self.texts[idx])&#10;        seq = [self.vocab.get(token, self.vocab['&lt;UNK&gt;']) for token in tokens]&#10;        seq = seq[:self.max_len] + [self.vocab['&lt;PAD&gt;']] * (self.max_len - len(seq))&#10;        return torch.tensor(seq), torch.tensor(self.labels[idx])" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/inference.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/inference.py" />
              <option name="originalContent" value="import pandas as pd&#10;import torch&#10;from data_utils import tokenize, build_vocab, TextDataset&#10;from model import SpamHamLSTM&#10;&#10;# Load the dataset (or create your own sample)&#10;sample_data = [&#10;    {'text': 'Congratulations! You have won a free ticket.', 'label': 'spam'},&#10;    {'text': 'Are we meeting today?', 'label': 'ham'},&#10;    {'text': 'Win cash now by clicking this link!', 'label': 'spam'},&#10;    {'text': 'Lunch at 1pm?', 'label': 'ham'}&#10;]&#10;df = pd.DataFrame(sample_data)&#10;texts = df['text'].tolist()&#10;labels = [1 if l == 'spam' else 0 for l in df['label']]&#10;&#10;# Build vocab and dataset&#10;vocab = build_vocab(texts)&#10;max_len = 50&#10;dataset = TextDataset(texts, labels, vocab, max_len)&#10;&#10;# Load model&#10;model = SpamHamLSTM(vocab_size=len(vocab), embed_dim=64, hidden_dim=128, num_classes=2)&#10;model.load_state_dict(torch.load('spamham_lstm.pt', map_location='cpu'))&#10;model.eval()&#10;&#10;# Inference&#10;with torch.no_grad():&#10;    for i in range(len(dataset)):&#10;        x, _ = dataset[i]&#10;        x = x.unsqueeze(0)&#10;        output = model(x)&#10;        pred = torch.argmax(output, dim=1).item()&#10;        label = 'spam' if pred == 1 else 'ham'&#10;        print(f&quot;Text: {texts[i]}\nPredicted: {label}\n&quot;)&#10;&#10;" />
              <option name="updatedContent" value="import pandas as pd&#10;import torch&#10;from data_utils import tokenize, build_vocab, TextDataset&#10;from model import SpamHamLSTM&#10;import csv&#10;&#10;# Load the training dataset to build vocab&#10;try:&#10;    data = pd.read_csv('spamhamdata.csv')&#10;    texts = data['text'].tolist()&#10;    labels = [1 if l == 'spam' else 0 for l in data['label']]&#10;except Exception:&#10;    texts, labels = [], []&#10;    with open('spamhamdata.csv', 'r', encoding='utf-8') as f:&#10;        reader = csv.reader(f, delimiter='\t')&#10;        for row in reader:&#10;            if len(row) &gt;= 2:&#10;                labels.append(1 if row[0].strip().lower() == 'spam' else 0)&#10;                texts.append(row[1].strip())&#10;&#10;vocab = build_vocab(texts)&#10;max_len = 50&#10;&#10;dataset = TextDataset(texts, labels, vocab, max_len)&#10;&#10;model = SpamHamLSTM(vocab_size=len(vocab), embed_dim=64, hidden_dim=128, num_classes=2)&#10;model.load_state_dict(torch.load('spamham_lstm.pt', map_location='cpu'))&#10;model.eval()&#10;&#10;with torch.no_grad():&#10;    for i in range(len(dataset)):&#10;        x, _ = dataset[i]&#10;        x = x.unsqueeze(0)&#10;        output = model(x)&#10;        pred = torch.argmax(output, dim=1).item()&#10;        label = 'spam' if pred == 1 else 'ham'&#10;        print(f&quot;Text: {texts[i]}\nPredicted: {label}\n&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/main.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/main.py" />
              <option name="originalContent" value="# PyTorch Spam-Ham Classifier (LSTM)&#10;&#10;import pandas as pd&#10;import torch&#10;from torch.utils.data import DataLoader&#10;from sklearn.model_selection import train_test_split&#10;from data_utils import tokenize, build_vocab, TextDataset&#10;from model import SpamHamLSTM&#10;import torch.nn as nn&#10;import torch.optim as optim&#10;&#10;# 1. Load Data&#10;&#10;data = pd.read_csv('spamhamdata.csv')&#10;texts = data['text'].tolist()&#10;labels = [1 if l == 'spam' else 0 for l in data['label']]&#10;&#10;vocab = build_vocab(texts)&#10;max_len = 50&#10;&#10;X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)&#10;train_dataset = TextDataset(X_train, y_train, vocab, max_len)&#10;test_dataset = TextDataset(X_test, y_test, vocab, max_len)&#10;train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)&#10;test_loader = DataLoader(test_dataset, batch_size=32)&#10;&#10;# 2. Training and Evaluation&#10;&#10;def train(model, loader, criterion, optimizer, device):&#10;    model.train()&#10;    total_loss = 0&#10;    for x, y in loader:&#10;        x, y = x.to(device), y.to(device)&#10;        optimizer.zero_grad()&#10;        output = model(x)&#10;        loss = criterion(output, y)&#10;        loss.backward()&#10;        optimizer.step()&#10;        total_loss += loss.item()&#10;    return total_loss / len(loader)&#10;&#10;def evaluate(model, loader, device):&#10;    model.eval()&#10;    correct = 0&#10;    total = 0&#10;    with torch.no_grad():&#10;        for x, y in loader:&#10;            x, y = x.to(device), y.to(device)&#10;            output = model(x)&#10;            preds = torch.argmax(output, dim=1)&#10;            correct += (preds == y).sum().item()&#10;            total += y.size(0)&#10;    return correct / total&#10;&#10;# 3. Main&#10;&#10;def main():&#10;    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')&#10;    model = SpamHamLSTM(vocab_size=len(vocab), embed_dim=64, hidden_dim=128, num_classes=2).to(device)&#10;    criterion = nn.CrossEntropyLoss()&#10;    optimizer = optim.Adam(model.parameters(), lr=0.001)&#10;    epochs = 5&#10;    for epoch in range(epochs):&#10;        train_loss = train(model, train_loader, criterion, optimizer, device)&#10;        acc = evaluate(model, test_loader, device)&#10;        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Test Acc={acc:.4f}')&#10;    torch.save(model.state_dict(), 'spamham_lstm.pt')&#10;    print('Model saved to spamham_lstm.pt')&#10;&#10;if __name__ == '__main__':&#10;    main()&#10;" />
              <option name="updatedContent" value="# PyTorch Spam-Ham Classifier (LSTM)&#10;&#10;import pandas as pd&#10;import torch&#10;from torch.utils.data import DataLoader&#10;from sklearn.model_selection import train_test_split&#10;from data_utils import tokenize, build_vocab, TextDataset&#10;from model import SpamHamLSTM&#10;import torch.nn as nn&#10;import torch.optim as optim&#10;import csv&#10;&#10;# 1. Load Data&#10;&#10;# Try to load as CSV first, fallback to tab-separated if error&#10;try:&#10;    data = pd.read_csv('spamhamdata.csv')&#10;    texts = data['text'].tolist()&#10;    labels = [1 if l == 'spam' else 0 for l in data['label']]&#10;except Exception:&#10;    # Fallback for tab-separated format (no header)&#10;    texts, labels = [], []&#10;    with open('spamhamdata.csv', 'r', encoding='utf-8') as f:&#10;        reader = csv.reader(f, delimiter='\t')&#10;        for row in reader:&#10;            if len(row) &gt;= 2:&#10;                labels.append(1 if row[0].strip().lower() == 'spam' else 0)&#10;                texts.append(row[1].strip())&#10;&#10;vocab = build_vocab(texts)&#10;max_len = 50&#10;&#10;X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)&#10;train_dataset = TextDataset(X_train, y_train, vocab, max_len)&#10;test_dataset = TextDataset(X_test, y_test, vocab, max_len)&#10;train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)&#10;test_loader = DataLoader(test_dataset, batch_size=32)&#10;&#10;# 2. Training and Evaluation&#10;&#10;def train(model, loader, criterion, optimizer, device):&#10;    model.train()&#10;    total_loss = 0&#10;    for x, y in loader:&#10;        x, y = x.to(device), y.to(device)&#10;        optimizer.zero_grad()&#10;        output = model(x)&#10;        loss = criterion(output, y)&#10;        loss.backward()&#10;        optimizer.step()&#10;        total_loss += loss.item()&#10;    return total_loss / len(loader)&#10;&#10;def evaluate(model, loader, device):&#10;    model.eval()&#10;    correct = 0&#10;    total = 0&#10;    with torch.no_grad():&#10;        for x, y in loader:&#10;            x, y = x.to(device), y.to(device)&#10;            output = model(x)&#10;            preds = torch.argmax(output, dim=1)&#10;            correct += (preds == y).sum().item()&#10;            total += y.size(0)&#10;    return correct / total&#10;&#10;# 3. Main&#10;&#10;def main():&#10;    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')&#10;    model = SpamHamLSTM(vocab_size=len(vocab), embed_dim=64, hidden_dim=128, num_classes=2).to(device)&#10;    criterion = nn.CrossEntropyLoss()&#10;    optimizer = optim.Adam(model.parameters(), lr=0.001)&#10;    epochs = 5&#10;    for epoch in range(epochs):&#10;        train_loss = train(model, train_loader, criterion, optimizer, device)&#10;        acc = evaluate(model, test_loader, device)&#10;        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f}, Test Acc={acc:.4f}')&#10;    torch.save(model.state_dict(), 'spamham_lstm.pt')&#10;    print('Model saved to spamham_lstm.pt')&#10;&#10;if __name__ == '__main__':&#10;    main()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/model.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/model.py" />
              <option name="updatedContent" value="import torch&#10;import torch.nn as nn&#10;&#10;class SpamHamLSTM(nn.Module):&#10;    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):&#10;        super().__init__()&#10;        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)&#10;        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)&#10;        self.fc = nn.Linear(hidden_dim, num_classes)&#10;&#10;    def forward(self, x):&#10;        x = self.embedding(x)&#10;        _, (h_n, _) = self.lstm(x)&#10;        out = self.fc(h_n[-1])&#10;        return out&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/requirements.txt">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/requirements.txt" />
              <option name="updatedContent" value="torch&#10;pandas&#10;scikit-learn&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>